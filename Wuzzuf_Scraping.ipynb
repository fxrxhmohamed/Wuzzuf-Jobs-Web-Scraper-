{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a94dcc7",
   "metadata": {},
   "source": [
    "#  Wuzzuf Scraping Engineering Jobs Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdf944",
   "metadata": {},
   "source": [
    "# Note:\n",
    "-> This code limits scrapping to only the first 7 pages if a specific job title has more than this just to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Page 1 ---\n",
      "\n",
      "--- Scraping Page 2 ---\n",
      "\n",
      "--- Scraping Page 3 ---\n",
      "\n",
      "--- Scraping Page 4 ---\n",
      "No more pages found.\n",
      "\n",
      "✅ Scraping completed. 49 jobs saved to JSON and CSV.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time, json, sys, csv\n",
    "\n",
    "# Initialize Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://wuzzuf.net/jobs/egypt\")\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Search for job\n",
    "search_box = wait.until(EC.element_to_be_clickable((By.NAME, \"q\")))\n",
    "time.sleep(2)\n",
    "search_box.click()\n",
    "search_box.clear() \n",
    "search_box.send_keys(\"Agricultural Engineer\")  # this is a small faster search that returns 49 results across 4 pages\n",
    "# search_box.send_keys(\"software engineering\")  \n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(5)\n",
    "\n",
    "all_jobs_data = []\n",
    "scraped_urls = set()  # To prevent duplicates\n",
    "current_page_number = 1\n",
    "MAX_PAGES = 7  # Maximum pages to scrape\n",
    "\n",
    "while True:\n",
    "    print(f\"\\n--- Scraping Page {current_page_number} ---\")\n",
    "    try:\n",
    "        # Scroll to bottom to load lazy-loaded jobs\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get all job cards\n",
    "        job_cards = wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.css-ghe2tq.e1v1l3u10\"))\n",
    "        )\n",
    "\n",
    "        # Extract job details\n",
    "        for job in job_cards:\n",
    "            try:\n",
    "                application_link = job.find_element(By.CSS_SELECTOR, \"h2.css-193uk2c a\").get_attribute(\"href\")\n",
    "                if application_link in scraped_urls:\n",
    "                    continue\n",
    "                scraped_urls.add(application_link)\n",
    "\n",
    "                title = job.find_element(By.CSS_SELECTOR, \"h2.css-193uk2c\").text.strip()\n",
    "                company = job.find_element(By.CSS_SELECTOR, \"a.css-ipsyv7\").text.strip()\n",
    "                location = job.find_element(By.CSS_SELECTOR, \"span.css-16x61xq\").text.strip()\n",
    "\n",
    "                try:\n",
    "                    exp_level = job.find_element(By.XPATH, \".//a[contains(@class,'css-o171kl')]/following-sibling::span\").text.strip()\n",
    "                except:\n",
    "                    exp_level = \"Not listed\"\n",
    "\n",
    "\n",
    "                try:\n",
    "                    date = job.find_element(By.CSS_SELECTOR, \"div.css-1jldrig\").text.strip()\n",
    "                except:\n",
    "                    date = \"Not listed\"\n",
    "\n",
    "\n",
    "                try:\n",
    "                    job_type = job.find_element(By.CSS_SELECTOR, \"span.css-uc9rga\").text.strip()\n",
    "                except:\n",
    "                    job_type = \"Not listed\"\n",
    "\n",
    "                # Open job detail page for skills\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", application_link)  # Opens job detail page in new tab.\n",
    "                driver.switch_to.window(driver.window_handles[1])  # Switches to that tab to scrape skills.\n",
    "                time.sleep(2)\n",
    "\n",
    "                try:\n",
    "                    skills_div = driver.find_element(By.CSS_SELECTOR, \"div.css-qe7mba\")\n",
    "                    skill_spans = skills_div.find_elements(By.CSS_SELECTOR, \"span.css-1vi25m1\")\n",
    "                    skills = \", \".join([s.text.strip() for s in skill_spans]) if skill_spans else \"Not listed\"\n",
    "                except:\n",
    "                    skills = \"Not listed\"\n",
    "\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0]) # Closes the current tab and switches back to main results page.\n",
    "\n",
    "                # Adds a dictionary for each job to all_jobs_data.\n",
    "                all_jobs_data.append({\n",
    "                    \"Job Title\": title,\n",
    "                    \"Company\": company,\n",
    "                    \"Location\": location,\n",
    "                    \"Experience Level\": exp_level,\n",
    "                    \"Date\": date,\n",
    "                    \"Skills\": skills,\n",
    "                    \"Job Type\": job_type,\n",
    "                    \"Application Link\": application_link,\n",
    "                })\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting job details: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "        # Pagination\n",
    "        # Stops if maximum pages reached.\n",
    "        if current_page_number >= MAX_PAGES:\n",
    "            print(f\"\\nReached the maximum page limit of {MAX_PAGES}. Stopping...\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Get all page buttons\n",
    "            page_buttons = driver.find_elements(By.CSS_SELECTOR, \"li.css-2t2q4i button\")\n",
    "            # variable to store which page number we are currently on\n",
    "            current_page = None\n",
    "\n",
    "            # Identify current active page\n",
    "            for idx, btn in enumerate(page_buttons):\n",
    "                if \"css-9ohdr1\" in btn.get_attribute(\"class\") or \"aria-current\" in btn.get_attribute(\"outerHTML\"):\n",
    "                    current_page = idx\n",
    "                    break\n",
    "\n",
    "            # If we successfully found the current page, and there is a next page available then click next page \n",
    "            if current_page is not None and current_page + 1 < len(page_buttons):\n",
    "                driver.execute_script(\"arguments[0].click();\", page_buttons[current_page + 1])\n",
    "                current_page_number += 1\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"No more pages found.\")\n",
    "                break  # Last page reached\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in pagination: {e}\", file=sys.stderr)\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading job cards: {e}\", file=sys.stderr)\n",
    "        break\n",
    "\n",
    "# Save results to JSON\n",
    "with open(\"wuzzuf_jobs_JSON.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_jobs_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save results to CSV\n",
    "with open(\"wuzzuf_jobs_CSV.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=all_jobs_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_jobs_data)\n",
    "\n",
    "print(f\"\\n✅ Scraping completed. {len(all_jobs_data)} jobs saved to JSON and CSV.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
